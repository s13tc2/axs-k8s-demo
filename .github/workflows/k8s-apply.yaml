name: k8s-Apply

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to run tests against'
        type: environment
        required: true

env:
  TERRAFORM_VERSION: '1.5.7'

jobs:

  infra:
    runs-on: ubuntu-latest
    outputs:
      kubernetes_cluster_name: ${{ steps.apply.outputs.kubernetes_cluster_name }}
      primary_region: ${{ steps.apply.outputs.primary_region }}
      console_role: ${{ steps.apply.outputs.console_role }}
      admin_group: ${{ steps.apply.outputs.admin_group }}
      alb_controller_role: ${{ steps.apply.outputs.alb_controller_role }}
      workload_identity_role: ${{ steps.apply.outputs.workload_identity_role }}

    environment:
      name: ${{ inputs.environment }}

    steps:
      - uses: actions/checkout@v3

      - id: setup
        name: Setup `terraform`
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - id: apply
        name: Terraform Apply
        env:
          AWS_ACCESS_KEY_ID: ${{ vars.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          BACKEND_BUCKET_NAME: ${{ vars.BUCKET_NAME }}
          BACKEND_REGION: ${{ vars.BUCKET_REGION }}
          BACKEND_KEY: ${{ vars.APPLICATION_NAME }}-${{ vars.ENVIRONMENT_NAME }}
        working-directory: ${{ vars.TERRAFORM_WORKING_DIRECTORY }}
        run: |
          terraform init \
            -backend-config='bucket='$BACKEND_BUCKET_NAME \
            -backend-config='region='$BACKEND_REGION \
            -backend-config="key=${{ vars.APPLICATION_NAME }}-${{ vars.ENVIRONMENT_NAME }}"

          terraform apply -target "random_shuffle.az" -auto-approve
          terraform apply -auto-approve

          kubernetes_cluster_name=$(terraform output -raw kubernetes_cluster_name)
          echo "kubernetes_cluster_name=$kubernetes_cluster_name" >> "$GITHUB_OUTPUT"

          primary_region=$(terraform output -raw primary_region)
          echo "primary_region=$primary_region" >> "$GITHUB_OUTPUT"

          console_role=$(terraform output -raw console_role_arn)
          echo "console_role=$console_role" >> "$GITHUB_OUTPUT"

          alb_controller_role=$(terraform output -raw alb_controller_role)
          echo "alb_controller_role=$alb_controller_role" >> "$GITHUB_OUTPUT"

          workload_identity_role=$(terraform output -raw workload_identity_role)
          echo "workload_identity_role=$workload_identity_role" >> "$GITHUB_OUTPUT"

          admin_group=$(terraform output -raw admin_group_arn)
          echo "admin_group=$admin_group" >> "$GITHUB_OUTPUT"

  k8s:
    runs-on: ubuntu-latest
    needs: infra

    environment:
      name: ${{ inputs.environment }}

    steps:
      - uses: actions/checkout@v3

      - id: setup
        name: Setup `terraform`
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ vars.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ needs.infra.outputs.primary_region }}

      - name: Update kubeconfig for EKS Cluster
        run: |
          aws eks update-kubeconfig --name ${{ needs.infra.outputs.kubernetes_cluster_name }} --region ${{ needs.infra.outputs.primary_region }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          kubectl version --client

      - name: Install Helm
        run: |
          curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
          chmod +x get_helm.sh
          ./get_helm.sh
          helm version

      - name: Add Helm Repositories
        run: |
          helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts
          helm repo add aws-secrets-store https://aws.github.io/secrets-store-csi-driver-provider-aws
          helm repo update

      - name: Install/Upgrade Secrets Store CSI Driver
        run: |
          helm upgrade --install csi-secrets-store secrets-store-csi-driver/secrets-store-csi-driver \
            --namespace kube-system \
            --create-namespace \
            --set syncSecret.enabled=true \
            --set installCRDs=true

      - name: Install/Upgrade AWS Secrets Provider
        run: |
          helm upgrade --install secrets-provider-aws aws-secrets-store/secrets-store-csi-driver-provider-aws \
            --namespace kube-system
          
      - name: Wait for CRDs to be Established
        run: |
          kubectl wait --for=condition=established crd secretproviderclasses.secrets-store.csi.x-k8s.io --timeout=120s
          kubectl wait --for=condition=established crd secretproviderclasspodstatuses.secrets-store.csi.x-k8s.io --timeout=120s

      # Ensure that you do NOT uninstall the Helm releases here
      # - name: Uninstall Existing Helm Releases
      #   run: |
      #     helm uninstall csi-secrets-store -n kube-system || true
      #     helm uninstall secrets-provider-aws -n kube-system || true

      - id: Apply
        name: Terraform Apply
        env:
          AWS_ACCESS_KEY_ID: ${{ vars.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ needs.infra.outputs.primary_region }}
          AWS_DEFAULT_REGION: ${{ needs.infra.outputs.primary_region }}
          BACKEND_BUCKET_NAME: ${{ vars.BUCKET_NAME }}
          BACKEND_REGION: ${{ vars.BUCKET_REGION }}
          BACKEND_KEY: ${{ vars.APPLICATION_NAME }}-${{ vars.ENVIRONMENT_NAME }}-k8s
          TF_VAR_application_name: ${{ vars.APPLICATION_NAME }}
          TF_VAR_environment_name: ${{ vars.ENVIRONMENT_NAME }}
          TF_VAR_cluster_name: ${{ needs.infra.outputs.kubernetes_cluster_name }}
          TF_VAR_primary_region: ${{ needs.infra.outputs.primary_region }}
          TF_VAR_alb_controller_role: ${{ needs.infra.outputs.alb_controller_role }}
          TF_VAR_workload_identity_role: ${{ needs.infra.outputs.workload_identity_role }}
          TF_VAR_secret_name: ${{ vars.APPLICATION_NAME }}-${{ vars.ENVIRONMENT_NAME }}-connection-string
          TF_VAR_secret_key: ${{ secrets.DB_CONNECTION_STRING_KEY }}
          TF_VAR_k8s_namespace: ${{ env.K8S_NAMESPACE }}
          TF_VAR_ingress_controller_namespace: ${{ env.INGRESS_CONTROLLER_NAMESPACE }}
        working-directory: ./src/terraform/k8s
        run: |
          terraform init \
            -backend-config='bucket='$BACKEND_BUCKET_NAME \
            -backend-config='region='$BACKEND_REGION \
            -backend-config="key=${{ vars.APPLICATION_NAME }}-${{ vars.ENVIRONMENT_NAME }}-k8s"
              
          terraform apply -auto-approve

          k8s_namespace=$(terraform output -raw k8s_namespace)
          echo "k8s_namespace=$k8s_namespace" >> "$GITHUB_OUTPUT"

      # Debugging Steps Start Here
      - name: Get Pods Status
        run: |
          kubectl get pods -n ${{ steps.Apply.outputs.k8s_namespace }} -o wide

      - name: Get Services
        run: |
          kubectl get services -n ${{ steps.Apply.outputs.k8s_namespace }}

      - name: Get Endpoints
        run: |
          kubectl get endpoints -n ${{ steps.Apply.outputs.k8s_namespace }}

      - name: Get Ingresses
        run: |
          kubectl get ingress -n ${{ steps.Apply.outputs.k8s_namespace }}

      - name: Describe Ingress
        run: |
          kubectl describe ingress $INGRESS_NAME -n ${{ steps.Apply.outputs.k8s_namespace }}
        env:
          INGRESS_NAME: ${{ steps.Apply.outputs.ingress_name }}

      - name: Get Ingress Classes
        run: |
          kubectl get ingressclass

      - name: Describe Ingress Class
        run: |
          kubectl describe ingressclass nginx

      - name: Verify IngressClass Controller
        run: |
          kubectl get ingressclass nginx -o yaml

      - name: List All ClusterRoles
        run: |
          kubectl get clusterrole

      - name: Get Cluster Roles
        run: |
          kubectl get clusterrole ingress-nginx-controller -o yaml

      - name: Describe ClusterRole
        run: |
          kubectl describe clusterrole ingress-nginx-controller

      - name: List All ClusterRoleBindings
        run: |
          kubectl get clusterrolebinding

      - name: Get Cluster Role Bindings
        run: |
          kubectl get clusterrolebinding ingress-nginx-controller -o yaml

      - name: Describe ClusterRoleBinding
        run: |
          kubectl describe clusterrolebinding ingress-nginx-controller

      - name: Check Ingress Controller ServiceAccount
        run: |
          kubectl get serviceaccount ingress-nginx -n ${{ steps.apply.outputs.k8s_namespace }} -o yaml

      - name: Describe ClusterRoleBinding for ServiceAccount
        run: |
          kubectl describe clusterrolebinding ingress-nginx-controller

      - name: Get Nginx Ingress Controller Pods and Labels
        run: |
          kubectl get pods -n ${{ steps.apply.outputs.k8s_namespace }} --show-labels

      - name: Get Nginx Ingress Controller Logs
        run: |
          for pod in $(kubectl get pods -n ${{ steps.apply.outputs.k8s_namespace }} -l app.kubernetes.io/name=nginx-ingress-controller -o jsonpath='{.items[*].metadata.name}'); do
            echo "Logs for Nginx Ingress Controller pod: $pod"
            kubectl logs $pod -n ${{ steps.apply.outputs.k8s_namespace }}
          done

      - name: Check Ingress Controller Logs for Errors
        run: |
          for pod in $(kubectl get pods -n ${{ steps.apply.outputs.k8s_namespace }} -l app.kubernetes.io/name=nginx-ingress-controller -o jsonpath='{.items[*].metadata.name}'); do
            echo "Errors and Warnings in Nginx Ingress Controller pod: $pod"
            kubectl logs $pod -n ${{ steps.apply.outputs.k8s_namespace }} | grep -i "error\|warning"
          done

      - name: Validate Endpoints After Changes
        run: |
          kubectl get endpoints -n ${{ steps.apply.outputs.k8s_namespace }}

      - name: Get Application Pod Logs
        run: |
          for pod in $(kubectl get pods -n ${{ steps.apply.outputs.k8s_namespace }} -l app=fleet-portal -o jsonpath='{.items[*].metadata.name}'); do
            echo "Logs for fleet-portal pod: $pod"
            kubectl logs $pod -n ${{ steps.apply.outputs.k8s_namespace }}
          done
          for pod in $(kubectl get pods -n ${{ steps.apply.outputs.k8s_namespace }} -l app=fleet-api -o jsonpath='{.items[*].metadata.name}'); do
            echo "Logs for fleet-api pod: $pod"
            kubectl logs $pod -n ${{ steps.apply.outputs.k8s_namespace }}
          done

      - name: Get Network Policies
        run: |
          kubectl get networkpolicy -A -o yaml

      - name: Get Events
        run: |
          kubectl get events -A --sort-by='.lastTimestamp'

      - name: Test Connectivity to fleet-portal-service
        run: |
          kubectl run test-pod --rm -i --tty --image=busybox --restart=Never -- /bin/sh -c "wget -qO- http://fleet-portal-service.${{ steps.apply.outputs.k8s_namespace }}.svc.cluster.local:80"

      - name: Test Connectivity to fleet-api-service
        run: |
          kubectl run test-pod --rm -i --tty --image=busybox --restart=Never -- /bin/sh -c "wget -qO- http://fleet-api-service.${{ steps.apply.outputs.k8s_namespace }}.svc.cluster.local:80"



      - name: Setup eksctl
        run: |
          # Set ARCH to the appropriate value if running on a different architecture
          ARCH=amd64
          PLATFORM=$(uname -s)_$ARCH

          # Download eksctl
          curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"

          # Optionally verify checksum
          curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check

          # Extract and move to a location in PATH
          tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          rm eksctl_$PLATFORM.tar.gz

          # Verify installation
          eksctl version

      - name: Test eksctl command
        run: |
          eksctl get clusters
          eksctl get iamidentitymapping \
            --cluster ${{ needs.infra.outputs.kubernetes_cluster_name }} \
            --region=${{ needs.infra.outputs.primary_region }}

          eksctl create iamidentitymapping \
            --cluster ${{ needs.infra.outputs.kubernetes_cluster_name }} \
            --region=${{ needs.infra.outputs.primary_region }} \
            --arn ${{ needs.infra.outputs.console_role }} \
            --group eks-console-dashboard-full-access-group \
            --no-duplicate-arns

          eksctl create iamidentitymapping \
            --cluster ${{ needs.infra.outputs.kubernetes_cluster_name }} \
            --region=${{ needs.infra.outputs.primary_region }} \
            --arn arn:aws:iam::312344499806:user/markti \
            --group eks-console-dashboard-restricted-access-group \
            --no-duplicate-arns
